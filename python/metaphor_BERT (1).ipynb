{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXtCFB4iuCMK"
      },
      "outputs": [],
      "source": [
        "##Parameter setting\n",
        "setEpoch = 30\n",
        "setLearningRate = 0.00002\n",
        "setEpsilon = 1e-8\n",
        "setBatch = 16\n",
        "setMaxLength = 128\n",
        "setSeed = 42 # random.seed\n",
        "setTry = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOIZociR6o7o"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSG1oQ8vBHxy"
      },
      "outputs": [],
      "source": [
        "!pip install ggplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVHzMTiuUQk2"
      },
      "outputs": [],
      "source": [
        "!pip install plotnine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFyxmk9P6eHZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import sentencepiece as spm\n",
        "% matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vfMW96MuJYr"
      },
      "outputs": [],
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "    \n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YmyFCc7xOh8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import sentencepiece as spm\n",
        "import collections\n",
        "import logging\n",
        "import unicodedata\n",
        "from io import open\n",
        "from shutil import copyfile\n",
        "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-pZG8_8xWEH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYUl4fruxhn9"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6dekj7Y9i_i"
      },
      "outputs": [],
      "source": [
        "!pip install tokoenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zlgf4Og8ZZo"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Tokenization classes.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import collections\n",
        "import logging\n",
        "import unicodedata\n",
        "from typing import List, Optional, Tuple\n",
        "import os\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from io import open\n",
        "from shutil import copyfile\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\":  \"vocab.txt\",\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    'vocab_file':\n",
        "    {\n",
        "        'bert-base-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt',\n",
        "        'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\",\n",
        "        'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\",\n",
        "        'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt\",\n",
        "        'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\",\n",
        "        'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt\",\n",
        "        'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\",\n",
        "        'bert-base-german-cased': \"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt\",\n",
        "        'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-vocab.txt\",\n",
        "        'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-vocab.txt\",\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-vocab.txt\",\n",
        "        'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-vocab.txt\",\n",
        "        'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-vocab.txt\",\n",
        "    },\n",
        "    'vocab_txt': \n",
        "    {\n",
        "        'bert-base-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt'\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    'bert-base-uncased': 512,\n",
        "    'bert-large-uncased': 512,\n",
        "    'bert-base-cased': 512,\n",
        "    'bert-large-cased': 512,\n",
        "    'bert-base-multilingual-uncased': 512,\n",
        "    'bert-base-multilingual-cased': 512,\n",
        "    'bert-base-chinese': 512,\n",
        "    'bert-base-german-cased': 512,\n",
        "    'bert-large-uncased-whole-word-masking': 512,\n",
        "    'bert-large-cased-whole-word-masking': 512,\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad': 512,\n",
        "    'bert-large-cased-whole-word-masking-finetuned-squad': 512,\n",
        "    'bert-base-cased-finetuned-mrpc': 512,\n",
        "}\n",
        "\n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    'bert-base-uncased': {'do_lower_case': True},\n",
        "    'bert-large-uncased': {'do_lower_case': True},\n",
        "    'bert-base-cased': {'do_lower_case': False},\n",
        "    'bert-large-cased': {'do_lower_case': False},\n",
        "    'bert-base-multilingual-uncased': {'do_lower_case': True},\n",
        "    'bert-base-multilingual-cased': {'do_lower_case': False},\n",
        "    'bert-base-chinese': {'do_lower_case': False},\n",
        "    'bert-base-german-cased': {'do_lower_case': False},\n",
        "    'bert-large-uncased-whole-word-masking': {'do_lower_case': True},\n",
        "    'bert-large-cased-whole-word-masking': {'do_lower_case': False},\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad': {'do_lower_case': True},\n",
        "    'bert-large-cased-whole-word-masking-finetuned-squad': {'do_lower_case': False},\n",
        "    'bert-base-cased-finetuned-mrpc': {'do_lower_case': False},\n",
        "}\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "        tokens = reader.readlines()\n",
        "    for index, token in enumerate(tokens):\n",
        "        token = token.rstrip('\\n')\n",
        "        vocab[token] = index\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "\n",
        "class BertTokenizer(PreTrainedTokenizer):\n",
        "    r\"\"\"\n",
        "    Constructs a BertTokenizer.\n",
        "    :class:`~pytorch_transformers.BertTokenizer` runs end-to-end tokenization: punctuation splitting + wordpiece\n",
        "\n",
        "    Args:\n",
        "        vocab_file: Path to a one-wordpiece-per-line vocabulary file\n",
        "        do_lower_case: Whether to lower case the input. Only has an effect when do_wordpiece_only=False\n",
        "        do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n",
        "        max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the\n",
        "            minimum of this value (if specified) and the underlying BERT model's sequence length.\n",
        "        never_split: List of tokens which will never be split during tokenization. Only has an effect when\n",
        "            do_wordpiece_only=False\n",
        "    \"\"\"\n",
        "\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(self, vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None,\n",
        "                 unk_token=\"[UNK]\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\",\n",
        "                 mask_token=\"[MASK]\", tokenize_chinese_chars=True, **kwargs):\n",
        "        \"\"\"Constructs a BertTokenizer.\n",
        "\n",
        "        Args:\n",
        "            **vocab_file**: Path to a one-wordpiece-per-line vocabulary file\n",
        "            **do_lower_case**: (`optional`) boolean (default True)\n",
        "                Whether to lower case the input\n",
        "                Only has an effect when do_basic_tokenize=True\n",
        "            **do_basic_tokenize**: (`optional`) boolean (default True)\n",
        "                Whether to do basic tokenization before wordpiece.\n",
        "            **never_split**: (`optional`) list of string\n",
        "                List of tokens which will never be split during tokenization.\n",
        "                Only has an effect when do_basic_tokenize=True\n",
        "            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n",
        "                Whether to tokenize Chinese characters.\n",
        "                This should likely be deactivated for Japanese:\n",
        "                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n",
        "        \"\"\"\n",
        "        super(BertTokenizer, self).__init__(unk_token=unk_token, sep_token=sep_token,\n",
        "                                            pad_token=pad_token, cls_token=cls_token,\n",
        "                                            mask_token=mask_token, **kwargs)\n",
        "        #self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n",
        "        #self.max_len_sentences_pair = self.max_len - 3  # take into account special tokens\n",
        "\n",
        "        if not os.path.isfile(vocab_file):\n",
        "            raise ValueError(\n",
        "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
        "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file))\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.ids_to_tokens = collections.OrderedDict(\n",
        "            [(ids, tok) for tok, ids in self.vocab.items()])\n",
        "        self.do_basic_tokenize = do_basic_tokenize\n",
        "        if do_basic_tokenize:\n",
        "            self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n",
        "                                                  never_split=never_split,\n",
        "                                                  tokenize_chinese_chars=tokenize_chinese_chars)\n",
        "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        split_tokens = []\n",
        "        if self.do_basic_tokenize:\n",
        "            for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
        "                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "                    split_tokens.append(sub_token)\n",
        "        else:\n",
        "            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n",
        "        return split_tokens\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.ids_to_tokens.get(index, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
        "        out_string = ' '.join(tokens).replace(' ##', '').strip()\n",
        "        return out_string\n",
        "\n",
        "\n",
        "    def add_special_tokens_single_sentence(self, token_ids):\n",
        "        \"\"\"\n",
        "        Adds special tokens to the a sequence for sequence classification tasks.\n",
        "        A BERT sequence has the following format: [CLS] X [SEP]\n",
        "        \"\"\"\n",
        "        return [self.cls_token_id] + token_ids + [self.sep_token_id]\n",
        "\n",
        "\n",
        "    def add_special_tokens_sentences_pair(self, token_ids_0, token_ids_1):\n",
        "        \"\"\"\n",
        "        Adds special tokens to a sequence pair for sequence classification tasks.\n",
        "        A BERT sequence pair has the following format: [CLS] A [SEP] B [SEP]\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A BERT sequence has the following format:\n",
        "\n",
        "        - single sequence: ``[CLS] X [SEP]``\n",
        "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
        "\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of IDs to which the special tokens will be added\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "\n",
        "        Returns:\n",
        "            :obj:`List[int]`: list of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "\n",
        "    def get_special_tokens_mask(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
        "\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of ids.\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "                Set to True if the token list is already formatted with special tokens for the model\n",
        "\n",
        "        Returns:\n",
        "            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A BERT sequence pair mask has the following format:\n",
        "\n",
        "        ::\n",
        "\n",
        "            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
        "            | first sequence    | second sequence |\n",
        "\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of ids.\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "\n",
        "        Returns:\n",
        "            :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n",
        "            sequence(s).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]    \n",
        "\n",
        "\n",
        "    def save_vocabulary(self, vocab_path):\n",
        "        \"\"\"Save the tokenizer vocabulary to a directory or file.\"\"\"\n",
        "        index = 0\n",
        "        if os.path.isdir(vocab_path):\n",
        "            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES['vocab_file'])\n",
        "        else:\n",
        "            vocab_file = vocab_path\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                                   \" Please check that the vocabulary is not corrupted!\".format(vocab_file))\n",
        "                    index = token_index\n",
        "                writer.write(token + u'\\n')\n",
        "                index += 1\n",
        "        return (vocab_file,)\n",
        "\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "    def __init__(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True):\n",
        "        \"\"\" Constructs a BasicTokenizer.\n",
        "\n",
        "        Args:\n",
        "            **do_lower_case**: Whether to lower case the input.\n",
        "            **never_split**: (`optional`) list of str\n",
        "                Kept for backward compatibility purposes.\n",
        "                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n",
        "                List of token not to split.\n",
        "            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n",
        "                Whether to tokenize Chinese characters.\n",
        "                This should likely be deactivated for Japanese:\n",
        "                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n",
        "        \"\"\"\n",
        "        if never_split is None:\n",
        "            never_split = []\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.never_split = never_split\n",
        "        self.tokenize_chinese_chars = tokenize_chinese_chars\n",
        "\n",
        "    def tokenize(self, text, never_split=None):\n",
        "        \"\"\" Basic Tokenization of a piece of text.\n",
        "            Split on \"white spaces\" only, for sub-word tokenization, see WordPieceTokenizer.\n",
        "\n",
        "        Args:\n",
        "            **never_split**: (`optional`) list of str\n",
        "                Kept for backward compatibility purposes.\n",
        "                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n",
        "                List of token not to split.\n",
        "        \"\"\"\n",
        "        never_split = self.never_split + (never_split if never_split is not None else [])\n",
        "        text = self._clean_text(text)\n",
        "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "        # models. This is also applied to the English models now, but it doesn't\n",
        "        # matter since the English models were not trained on any Chinese data\n",
        "        # and generally don't have any Chinese data in them (there are Chinese\n",
        "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "        # words in the English Wikipedia.).\n",
        "        if self.tokenize_chinese_chars:\n",
        "            text = self._tokenize_chinese_chars(text)\n",
        "        orig_tokens = whitespace_tokenize(text)\n",
        "        split_tokens = []\n",
        "        for token in orig_tokens:\n",
        "            if self.do_lower_case and token not in never_split:\n",
        "                token = token.lower()\n",
        "                token = self._run_strip_accents(token)\n",
        "            split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "        return output_tokens\n",
        "\n",
        "    def _run_strip_accents(self, text):\n",
        "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "        text = unicodedata.normalize(\"NFD\", text)\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cat = unicodedata.category(char)\n",
        "            if cat == \"Mn\":\n",
        "                continue\n",
        "            output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _run_split_on_punc(self, text, never_split=None):\n",
        "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "        if never_split is not None and text in never_split:\n",
        "            return [text]\n",
        "        chars = list(text)\n",
        "        i = 0\n",
        "        start_new_word = True\n",
        "        output = []\n",
        "        while i < len(chars):\n",
        "            char = chars[i]\n",
        "            if _is_punctuation(char):\n",
        "                output.append([char])\n",
        "                start_new_word = True\n",
        "            else:\n",
        "                if start_new_word:\n",
        "                    output.append([])\n",
        "                start_new_word = False\n",
        "                output[-1].append(char)\n",
        "            i += 1\n",
        "\n",
        "        return [\"\".join(x) for x in output]\n",
        "\n",
        "    def _tokenize_chinese_chars(self, text):\n",
        "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if self._is_chinese_char(cp):\n",
        "                output.append(\" \")\n",
        "                output.append(char)\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _is_chinese_char(self, cp):\n",
        "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "        #\n",
        "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "        # space-separated words, so they are not treated specially and handled\n",
        "        # like the all of the other languages.\n",
        "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "                continue\n",
        "            if _is_whitespace(char):\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n",
        "        self.vocab = vocab\n",
        "        self.unk_token = unk_token\n",
        "        self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "\n",
        "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "        using the given vocabulary.\n",
        "\n",
        "        For example:\n",
        "          input = \"unaffable\"\n",
        "          output = [\"un\", \"##aff\", \"##able\"]\n",
        "\n",
        "        Args:\n",
        "          text: A single token or whitespace separated tokens. This should have\n",
        "            already been passed through `BasicTokenizer`.\n",
        "\n",
        "        Returns:\n",
        "          A list of wordpiece tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        output_tokens = []\n",
        "        for token in whitespace_tokenize(text):\n",
        "            chars = list(token)\n",
        "            if len(chars) > self.max_input_chars_per_word:\n",
        "                output_tokens.append(self.unk_token)\n",
        "                continue\n",
        "\n",
        "            is_bad = False\n",
        "            start = 0\n",
        "            sub_tokens = []\n",
        "            while start < len(chars):\n",
        "                end = len(chars)\n",
        "                cur_substr = None\n",
        "                while start < end:\n",
        "                    substr = \"\".join(chars[start:end])\n",
        "                    if start > 0:\n",
        "                        substr = \"##\" + substr\n",
        "                    if substr in self.vocab:\n",
        "                        cur_substr = substr\n",
        "                        break\n",
        "                    end -= 1\n",
        "                if cur_substr is None:\n",
        "                    is_bad = True\n",
        "                    break\n",
        "                sub_tokens.append(cur_substr)\n",
        "                start = end\n",
        "\n",
        "            if is_bad:\n",
        "                output_tokens.append(self.unk_token)\n",
        "            else:\n",
        "                output_tokens.extend(sub_tokens)\n",
        "        return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "    # as whitespace since they are generally considered as such.\n",
        "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat == \"Zs\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "    # These are technically control characters but we count them as whitespace\n",
        "    # characters.\n",
        "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return False\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"C\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "    cp = ord(char)\n",
        "    # We treat all non-letter/number ASCII as punctuation.\n",
        "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "    # Punctuation class but we treat them as punctuation anyways, for\n",
        "    # consistency.\n",
        "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "class BertTokenizerFast(PreTrainedTokenizerFast):\n",
        "    r\"\"\"\n",
        "    Constructs a \"Fast\" BERT tokenizer (backed by HuggingFace's `tokenizers` library).\n",
        "\n",
        "    Bert tokenization is Based on WordPiece.\n",
        "\n",
        "    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizerFast` which contains most of the methods. Users\n",
        "    should refer to the superclass for more information regarding methods.\n",
        "\n",
        "    Args:\n",
        "        vocab_file (:obj:`string`):\n",
        "            File containing the vocabulary.\n",
        "        do_lower_case (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
        "            Whether to lowercase the input when tokenizing.\n",
        "        unk_token (:obj:`string`, `optional`, defaults to \"[UNK]\"):\n",
        "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
        "            token instead.\n",
        "        sep_token (:obj:`string`, `optional`, defaults to \"[SEP]\"):\n",
        "            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences\n",
        "            for sequence classification or for a text and a question for question answering.\n",
        "            It is also used as the last token of a sequence built with special tokens.\n",
        "        pad_token (:obj:`string`, `optional`, defaults to \"[PAD]\"):\n",
        "            The token used for padding, for example when batching sequences of different lengths.\n",
        "        cls_token (:obj:`string`, `optional`, defaults to \"[CLS]\"):\n",
        "            The classifier token which is used when doing sequence classification (classification of the whole\n",
        "            sequence instead of per-token classification). It is the first token of the sequence when built with\n",
        "            special tokens.\n",
        "        mask_token (:obj:`string`, `optional`, defaults to \"[MASK]\"):\n",
        "            The token used for masking values. This is the token used when training this model with masked language\n",
        "            modeling. This is the token which the model will try to predict.\n",
        "        tokenize_chinese_chars (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
        "            Whether to tokenize Chinese characters.\n",
        "            This should likely be deactivated for Japanese:\n",
        "            see: https://github.com/huggingface/transformers/issues/328\n",
        "        clean_text (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
        "            Whether to clean the text before tokenization by removing any control characters and\n",
        "            replacing all whitespaces by the classic one.\n",
        "        tokenize_chinese_chars (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
        "            Whether to tokenize Chinese characters.\n",
        "            This should likely be deactivated for Japanese:\n",
        "            see: https://github.com/huggingface/transformers/issues/328\n",
        "    \"\"\"\n",
        "\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_file,\n",
        "        do_lower_case=True,\n",
        "        unk_token=\"[UNK]\",\n",
        "        sep_token=\"[SEP]\",\n",
        "        pad_token=\"[PAD]\",\n",
        "        cls_token=\"[CLS]\",\n",
        "        mask_token=\"[MASK]\",\n",
        "        clean_text=True,\n",
        "        tokenize_chinese_chars=True,\n",
        "        strip_accents=True,\n",
        "        wordpieces_prefix=\"##\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            BertWordPieceTokenizer(\n",
        "                vocab_file=vocab_file,\n",
        "                unk_token=unk_token,\n",
        "                sep_token=sep_token,\n",
        "                cls_token=cls_token,\n",
        "                clean_text=clean_text,\n",
        "                handle_chinese_chars=tokenize_chinese_chars,\n",
        "                strip_accents=strip_accents,\n",
        "                lowercase=do_lower_case,\n",
        "                wordpieces_prefix=wordpieces_prefix,\n",
        "            ),\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        self.do_lower_case = do_lower_case\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        output = [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "\n",
        "        if token_ids_1:\n",
        "            output += token_ids_1 + [self.sep_token_id]\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtKkaMcYNVZf"
      },
      "outputs": [],
      "source": [
        "for currentTry in range(2,setTry):\n",
        "  postpositions = [\"metaphor\"] #metaphor\n",
        "  labelNumber = 0\n",
        "  \n",
        "  for postposition in postpositions:\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    #test\n",
        "    fileDir = \"\"+postposition+\"_testt.csv\"\n",
        "    #fileDir = \"\"+postposition+\".csv\"\n",
        "    fr = open(fileDir, 'r')\n",
        "    contents= fr.readlines()\n",
        "    fr.close()\n",
        "\n",
        "    test = pd.DataFrame(columns=('index', 'label', 'sentence'))\n",
        "    i = 0\n",
        "    index = \"\"\n",
        "    # sentence_source = \"\"\n",
        "    label = \"\"\n",
        "    sentence = \"\"\n",
        "    for content in contents:\n",
        "        if i == 0:\n",
        "            pass\n",
        "        else:\n",
        "            infos = content.split(\",\")\n",
        "            index = infos[0]\n",
        "            label = int(infos[1])\n",
        "            sentence = infos[2].replace(\"\\n\",\"\")\n",
        "            test.loc[i] = [index, label, sentence]\n",
        "        i = i + 1\n",
        "    \n",
        "    #train\n",
        "    fileDir = \"\"+postposition+\"_trainn.csv\"\n",
        "    fr = open(fileDir, 'r')\n",
        "    contents= fr.readlines()\n",
        "    fr.close()\n",
        "\n",
        "    train = pd.DataFrame(columns=('index', 'label', 'sentence'))\n",
        "    i = 0\n",
        "    index = \"\"\n",
        "    # sentence_source = \"\"\n",
        "    label = \"\"\n",
        "    sentence = \"\"\n",
        "    for content in contents:\n",
        "        if i == 0:\n",
        "            pass\n",
        "        else:\n",
        "            infos = content.split(\",\")\n",
        "            index = infos[0]\n",
        "            label = int(infos[1])\n",
        "            sentence = infos[2].replace(\"\\n\",\"\")\n",
        "            train.loc[i] = [index, label, sentence]\n",
        "        i = i + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kaWVHR8ZdI6"
      },
      "outputs": [],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8xyL3rneA7v"
      },
      "outputs": [],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJGpWDl0zUBS"
      },
      "outputs": [],
      "source": [
        "  print(\"Installing transformers\")\n",
        "  !pip -qq install transformers\n",
        "  \n",
        "from transformers import BertModel, BertConfig\n",
        "configuration = BertConfig()\n",
        "\n",
        "# Initializing a model from the bert-base-uncased style configuration\n",
        "model = BertModel(configuration)\n",
        "\n",
        "# Accessing the model configuration\n",
        "configuration = model.config\n",
        "print(configuration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRSEhoRK483E"
      },
      "outputs": [],
      "source": [
        "    #Data Preprocessing\n",
        "\n",
        "    # train['sentence'] = train['sentence'].str.replace(r'[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》\\\\n\\t]+', \" \", regex=True)\n",
        "    # test['sentence'] = test['sentence'].str.replace(r'[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', \" \", regex=True)\n",
        "    # train['sentence'] = train['sentence'].str.replace(r'\\t+', \" \", regex=True)\n",
        "    # test['sentence'] = test['sentence'].str.replace(r'\\t+', \" \", regex=True)\n",
        "    # train['sentence'] = train['sentence'].str.replace(r'[\\\\n]+',\" \", regex=True)\n",
        "    # train['sentence'] = train['sentence'].str.replace(r'[\\s]+', \" \", regex=True)\n",
        "    # train['sentence'] = train['sentence'].str.strip()\n",
        "    # test['sentence'] = test['sentence'].str.replace(r'[\\\\n]+',\" \", regex=True)\n",
        "    # test['sentence'] = test['sentence'].str.replace(r'[\\s]+', \" \", regex=True)\n",
        "    # test['sentence'] = test['sentence'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUNBHcIC5K3m"
      },
      "outputs": [],
      "source": [
        "    # Train Sentence Extraction\n",
        "    sentences = train['sentence']\n",
        "    # Convert to the type of input in the BERT\n",
        "    sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
        "\n",
        "    # Label Extraction\n",
        "    labels = train['label'].values\n",
        "    labels_re = []\n",
        "    for label in labels:\n",
        "      labels_re.append(label)\n",
        "    labels = labels_re\n",
        "\n",
        "    # Activating the BERT Tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "    print(tokenized_texts[0])\n",
        "\n",
        "    # setMaxLength\n",
        "    MAX_LEN = setMaxLength\n",
        "\n",
        "    # Convert Tokens to Numeric Index\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "    # Cut the sentence to MAX_LEN length and fill the missing part with padding 0\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # Attention Mask Initialization\n",
        "    attention_masks = []\n",
        "\n",
        "    # Set the attention mask to 1 if it's not padding and 0 if it's padding\n",
        "    # The padding part does not perform attention on the BERT model, which speeds up\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # Seperate Train, Validation Sets\n",
        "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
        "                                                                                        labels, \n",
        "                                                                                        random_state=2018, \n",
        "                                                                                        test_size=0.2)\n",
        "\n",
        "    # Separatie the attention mask into the training set and the verification set\n",
        "    train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
        "                                                          input_ids,\n",
        "                                                          random_state=2018, \n",
        "                                                          test_size=0.2)\n",
        "\n",
        "    # Convert data to tensors in the pie torch\n",
        "    train_inputs = torch.tensor(train_inputs)\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    train_masks = torch.tensor(train_masks)\n",
        "    validation_inputs = torch.tensor(validation_inputs)\n",
        "    validation_labels = torch.tensor(validation_labels)\n",
        "    validation_masks = torch.tensor(validation_masks)\t\t\n",
        "\n",
        "    # Batch Size\n",
        "    batch_size = setBatch\n",
        "\n",
        "    # Set data by combining input, mask, and label with data loader of pie torch\n",
        "    # Import data by batch size during learning\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "    validation_sampler = SequentialSampler(validation_data)\n",
        "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD0T6zrH5VPV"
      },
      "outputs": [],
      "source": [
        "    # Test Sentence Extraction\n",
        "    sentences = test['sentence']\n",
        "\n",
        "    # Convert to the type of input in the BERT\n",
        "    sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
        "\n",
        "    # Label Extraction\n",
        "    labels = test['label'].values\n",
        "    labels_re = []\n",
        "    for label in labels:\n",
        "      labels_re.append(label)\n",
        "    labels = labels_re\n",
        "\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # Convert Tokens to Numeric Index\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "    # Cut the sentence to MAX_LEN length and fill the missing part with padding 0\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # Attention Mask Initialization\n",
        "    attention_masks = []\n",
        "\n",
        "    # Set the attention mask to 1 if it's not padding and 0 if it's padding\n",
        "    # The padding part does not perform attention on the BERT model, which speeds up\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # Convert data to tensors in the pytorch\n",
        "    test_inputs = torch.tensor(input_ids)\n",
        "    test_labels = torch.tensor(labels)\n",
        "    test_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    # Set data by combining input, mask, and label with data loader of pytorch\n",
        "    # Import data by batch size during learning\n",
        "    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "    test_sampler = RandomSampler(test_data)\n",
        "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn0O09h9_fHE"
      },
      "outputs": [],
      "source": [
        "      # Create a BERT Model for Classification\n",
        "      model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
        "      model.cuda() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlXPsxeO_5lL"
      },
      "outputs": [],
      "source": [
        "      # Accuracy Calculate Function\n",
        "      def flat_accuracy(preds, labels):\n",
        "          \n",
        "          pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "          labels_flat = labels.flatten()\n",
        "\n",
        "          return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "          \n",
        "      def TAR_flat_accuracy(preds, labels):\n",
        "          \n",
        "          pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "          labels_flat = labels.flatten()\n",
        "\n",
        "          match_num = 0\n",
        "          func_num = 0\n",
        "          for i in range(0,len(pred_flat)):\n",
        "            if (pred_flat[i] == labels_flat[i]) and (labels_flat[i] == 0):\n",
        "              match_num += 1\n",
        "            if labels_flat[i] == 0:\n",
        "              func_num += 1\n",
        "\n",
        "          if match_num == 0 or func_num == 0:\n",
        "            return 0\n",
        "          else:\n",
        "            return match_num / func_num\n",
        "\n",
        "      def INT_flat_accuracy(preds, labels):\n",
        "          \n",
        "          pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "          labels_flat = labels.flatten()\n",
        "\n",
        "          match_num = 0\n",
        "          func_num = 0\n",
        "          for i in range(0,len(pred_flat)):\n",
        "            if (pred_flat[i] == labels_flat[i]) and (labels_flat[i] == 1):\n",
        "              match_num += 1\n",
        "            if labels_flat[i] == 1:\n",
        "              func_num += 1\n",
        "\n",
        "          if match_num == 0 or func_num == 0:\n",
        "            return 0\n",
        "          else:\n",
        "            return match_num / func_num\n",
        "\n",
        "      def REP_flat_accuracy(preds, labels):\n",
        "          \n",
        "          pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "          labels_flat = labels.flatten()\n",
        "\n",
        "          match_num = 0\n",
        "          func_num = 0\n",
        "          for i in range(0,len(pred_flat)):\n",
        "            if (pred_flat[i] == labels_flat[i]) and (labels_flat[i] == 2):\n",
        "              match_num += 1\n",
        "            if labels_flat[i] == 2:\n",
        "              func_num += 1\n",
        "\n",
        "          if match_num == 0 or func_num == 0:\n",
        "            return 0\n",
        "          else:\n",
        "            return match_num / func_num\n",
        "\n",
        "      def VAR_flat_accuracy(preds, labels):\n",
        "          \n",
        "          pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "          labels_flat = labels.flatten()\n",
        "\n",
        "          match_num = 0\n",
        "          func_num = 0\n",
        "          for i in range(0,len(pred_flat)):\n",
        "            if (pred_flat[i] == labels_flat[i]) and (labels_flat[i] == 3):\n",
        "              match_num += 1\n",
        "            if labels_flat[i] == 3:\n",
        "              func_num += 1\n",
        "\n",
        "          if match_num == 0 or func_num == 0:\n",
        "            return 0\n",
        "          else:\n",
        "            return match_num / func_num\n",
        "\n",
        "      def TEC_flat_accuracy(preds, labels):\n",
        "          \n",
        "          pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "          labels_flat = labels.flatten()\n",
        "\n",
        "          match_num = 0\n",
        "          func_num = 0\n",
        "          for i in range(0,len(pred_flat)):\n",
        "            if (pred_flat[i] == labels_flat[i]) and (labels_flat[i] == 4):\n",
        "              match_num += 1\n",
        "            if labels_flat[i] == 4:\n",
        "              func_num += 1\n",
        "\n",
        "          if match_num == 0 or func_num == 0:\n",
        "            return 0\n",
        "          else:\n",
        "            return match_num / func_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp8g7jdZAVO-"
      },
      "outputs": [],
      "source": [
        "# Format Time Function\n",
        "def format_time(elapsed):\n",
        "      # Round\n",
        "      elapsed_rounded = int(round((elapsed)))\n",
        "          \n",
        "      # hh:mm:ss\n",
        "      return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNzL4XZJAi75"
      },
      "outputs": [],
      "source": [
        "      # Set Optimizer\n",
        "      optimizer = AdamW(model.parameters(),\n",
        "                        lr = setLearningRate, # Learning Rate\n",
        "                        eps = setEpsilon # Epsilon\n",
        "                      )\n",
        "      \n",
        "      param_optimizer = list(model.named_parameters())\n",
        "      no_decay = ['bias', 'LayerNorm.weight']\n",
        "      # Separate the `weight` parameters from the `bias` parameters. \n",
        "      # - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. \n",
        "      # - For the `bias` parameters, the 'weight_decay_rate' is 0.0. \n",
        "      optimizer_grouped_parameters = [\n",
        "          # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
        "          {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "           'weight_decay_rate': 0.1},\n",
        "    \n",
        "          # Filter for parameters which *do* include those.\n",
        "          {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "           'weight_decay_rate': 0.0}\n",
        "      ]\n",
        "\n",
        "\n",
        "      # Epoch\n",
        "      epochs = setEpoch\n",
        "\n",
        "      # Total Step\n",
        "      total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "      # Create a scheduler that reduces learning rates little by little\n",
        "      scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                  num_warmup_steps = 0,\n",
        "                                                  num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EMiHujBeWXP"
      },
      "outputs": [],
      "source": [
        "      # RandomSeed\n",
        "      seed_val = setSeed\n",
        "      random.seed(seed_val)\n",
        "      np.random.seed(seed_val)\n",
        "      torch.manual_seed(seed_val)\n",
        "      torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "      # Gradient Initialize\n",
        "      model.zero_grad()\n",
        "\n",
        "      final_info = {}\n",
        "\n",
        "      f = open(\"/result.txt\", 'w')\n",
        "      f.write(\"epoch,sentence,originallabel,predictedlabel,predictedFunction,result\"+\"\\n\")\n",
        "\n",
        "      # Repeat as Epoch\n",
        "      for epoch_i in range(1, epochs):\n",
        "          \n",
        "          # ========================================\n",
        "          #               Training\n",
        "          # ========================================\n",
        "          \n",
        "          print(\"\")\n",
        "          print('======== Epoch {:} / {:} ========'.format(epoch_i, epochs))\n",
        "          print('Training...')\n",
        "\n",
        "          # Set Start Time\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Loss Initialize\n",
        "          total_loss = 0\n",
        "\n",
        "          # Change to Train Mode\n",
        "          model.train()\n",
        "\n",
        "          # Tracking variables\n",
        "          tr_loss = 0\n",
        "          nb_tr_examples, nb_tr_steps = 0, 0\n",
        "              \n",
        "          # Import repeatedly by batch from the data loader\n",
        "          for step, batch in enumerate(train_dataloader):\n",
        "              # Display Progress Information\n",
        "              if step % 500 == 0 and not step == 0:\n",
        "                  elapsed = format_time(time.time() - t0)\n",
        "                  print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "              # Batch GPU\n",
        "              batch = tuple(t.to(device) for t in batch)\n",
        "              \n",
        "              # Extract data from batches\n",
        "              b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "              # Forward                 \n",
        "              outputs = model(b_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=b_input_mask, \n",
        "                              labels=b_labels)\n",
        "              \n",
        "              # Get Loss\n",
        "              loss = outputs[0]\n",
        "             \n",
        "              # Total Loss Calculate\n",
        "              total_loss += loss.item()\n",
        "\n",
        "              # Calculate gradients by performing Backward\n",
        "              loss.backward()\n",
        "\n",
        "              # Gradient Clipping\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "              # Update weight parameters via gradient\n",
        "              optimizer.step()\n",
        "\n",
        "              # Less Learning with Scheduler\n",
        "              scheduler.step()\n",
        "\n",
        "              # Update tracking variables\n",
        "              tr_loss += loss.item()\n",
        "              nb_tr_examples += b_input_ids.size(0)\n",
        "              nb_tr_steps += 1\n",
        "\n",
        "              print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "              # Validation\n",
        "\n",
        "              # Put model in evaluation mode to evaluate loss on the validation set\n",
        "              model.eval()\n",
        "\n",
        "              # Tracking variables \n",
        "              eval_loss, eval_accuracy = 0, 0\n",
        "              nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "              # Evaluate data for one epoch\n",
        "              for batch in validation_dataloader:\n",
        "                # Add batch to GPU\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                # Unpack the inputs from our dataloader\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "                # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "                with torch.no_grad():\n",
        "                  # Forward pass, calculate logit predictions\n",
        "                  logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "                # Move logits and labels to CPU\n",
        "                logits = logits['logits'].detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "                tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "                eval_accuracy += tmp_eval_accuracy\n",
        "                nb_eval_steps += 1\n",
        "\n",
        "                print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "              # Gradient Initialization\n",
        "              model.zero_grad()\n",
        "\n",
        "          # Calculate Avg_loss\n",
        "          avg_train_loss = total_loss / len(train_dataloader)            \n",
        "\n",
        "          print(\"\")\n",
        "          print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "          print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "              \n",
        "          # ========================================\n",
        "          #               Validation\n",
        "          # ========================================\n",
        "\n",
        "          print(\"\")\n",
        "          print(\"Running Validation...\")\n",
        "\n",
        "          # Set Start Time\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Change to Validation Mode\n",
        "          model.eval()\n",
        "\n",
        "          # Variable Initialization\n",
        "          eval_loss, eval_accuracy = 0, 0\n",
        "          nb_eval_steps, nb_eval_examples = 0, 0\n",
        "          FNS_nb_eval_steps, FNS_eval_accuracy = 0, 0\n",
        "          INS_nb_eval_steps, INS_eval_accuracy = 0, 0\n",
        "          DIR_nb_eval_steps, DIR_eval_accuracy = 0, 0\n",
        "          EFF_nb_eval_steps, EFF_eval_accuracy = 0, 0\n",
        "          CRT_nb_eval_steps, CRT_eval_accuracy = 0, 0\n",
        "\n",
        "\n",
        "          epoch_info = {}\n",
        "\n",
        "          # Import repeatedly by batch from the data loader\n",
        "          for batch in test_dataloader:\n",
        "              # Batch GPU\n",
        "              batch = tuple(t.to(device) for t in batch)\n",
        "              \n",
        "              # Extract data from batches\n",
        "              b_input_ids, b_input_mask, b_labels = batch              \n",
        "              \n",
        "              with torch.no_grad():     \n",
        "                  \n",
        "                  outputs = model(b_input_ids, \n",
        "                                  token_type_ids=None, \n",
        "                                  attention_mask=b_input_mask)\n",
        "              \n",
        "              # Get logit\n",
        "              logits = outputs[0]\n",
        "\n",
        "              # Move data to CPU\n",
        "              logits = logits.detach().cpu().numpy()\n",
        "              label_ids = b_labels.to('cpu').numpy()\n",
        "              \n",
        "              # Calculate accuracy by comparing output logits and labels\n",
        "              tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "              eval_accuracy += tmp_eval_accuracy\n",
        "              nb_eval_steps += 1\n",
        "\n",
        "              FNS_tmp_eval_accuracy = TAR_flat_accuracy(logits, label_ids)\n",
        "              FNS_eval_accuracy += FNS_tmp_eval_accuracy\n",
        "              FNS_nb_eval_steps += 1\n",
        "\n",
        "              INS_tmp_eval_accuracy = INT_flat_accuracy(logits, label_ids)\n",
        "              INS_eval_accuracy += INS_tmp_eval_accuracy\n",
        "              INS_nb_eval_steps += 1\n",
        "\n",
        "              DIR_tmp_eval_accuracy = REP_flat_accuracy(logits, label_ids)\n",
        "              DIR_eval_accuracy += DIR_tmp_eval_accuracy\n",
        "              DIR_nb_eval_steps += 1\n",
        "\n",
        "              EFF_tmp_eval_accuracy = VAR_flat_accuracy(logits, label_ids)\n",
        "              EFF_eval_accuracy += EFF_tmp_eval_accuracy\n",
        "              EFF_nb_eval_steps += 1\n",
        "\n",
        "              CRT_tmp_eval_accuracy = TEC_flat_accuracy(logits, label_ids)\n",
        "              CRT_eval_accuracy += CRT_tmp_eval_accuracy\n",
        "              CRT_nb_eval_steps += 1\n",
        "\n",
        "          print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "          print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "          print(\"\")\n",
        "          print(\"  Detail accuracy  \")\n",
        "          print(\"  Target_Accuracy: {0:.2f}\".format(FNS_eval_accuracy/FNS_nb_eval_steps))\n",
        "          print(\"  Intermediation_Accuracy: {0:.2f}\".format(INS_eval_accuracy/INS_nb_eval_steps))\n",
        "          print(\"  Representation_Accuracy: {0:.2f}\".format(DIR_eval_accuracy/DIR_nb_eval_steps))\n",
        "          print(\"  Visual_Variables_Accuracy: {0:.2f}\".format(EFF_eval_accuracy/EFF_nb_eval_steps))\n",
        "          print(\"  Visual_Techniques_Accuracy: {0:.2f}\".format(CRT_eval_accuracy/CRT_nb_eval_steps))\n",
        "\n",
        "          epoch_info[\"Total\"] = round(eval_accuracy/nb_eval_steps,3)\n",
        "          epoch_info[\"Loss\"] = round(avg_train_loss,3)\n",
        "          epoch_info[\"Tar\"] = round(FNS_eval_accuracy/FNS_nb_eval_steps,3)\n",
        "          epoch_info[\"Int\"] = round(INS_eval_accuracy/INS_nb_eval_steps,3)\n",
        "          epoch_info[\"Rep\"] = round(DIR_eval_accuracy/DIR_nb_eval_steps,3)\n",
        "          epoch_info[\"Var\"] = round(EFF_eval_accuracy/EFF_nb_eval_steps,3)\n",
        "          epoch_info[\"Tec\"] = round(CRT_eval_accuracy/CRT_nb_eval_steps,3)\n",
        "\n",
        "          final_info[\"epoch\"+str(epoch_i)] = epoch_info\n",
        "\n",
        "\n",
        "          # Change to evaluate mode\n",
        "          model.eval()\n",
        "          test_input_ids = []\n",
        "          test_input_mask = []\n",
        "          test_labels = []\n",
        "\n",
        "          num = 0\n",
        "          for step, batch in enumerate(test_data):   #467, 128\n",
        "            # print(\"batch\",batch)\n",
        "          \n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            \n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            input_ids_arr = []\n",
        "            input_mask_arr = []\n",
        "\n",
        "            \n",
        "\n",
        "            for i in range(0,len(b_input_ids)):\n",
        "              input_ids_arr.append(int(b_input_ids[i]))\n",
        "              input_mask_arr.append(int(b_input_mask[i]))\n",
        "\n",
        "            \n",
        "            test_input_ids.append(input_ids_arr)\n",
        "            test_input_mask.append(input_mask_arr)\n",
        "            test_labels.append(int(b_labels))\n",
        "\n",
        "\n",
        "          test_input_ids = torch.tensor(test_input_ids)\n",
        "          test_input_mask = torch.tensor(test_input_mask)\n",
        "          test_labels = test_labels\n",
        "\n",
        "          test_input_ids = test_input_ids.to(device)\n",
        "          test_input_mask = test_input_mask.to(device)\n",
        "\n",
        "\n",
        "          # No calculate gradient\n",
        "          with torch.no_grad():     \n",
        "              # Do Forward\n",
        "              outputs = model(test_input_ids, \n",
        "                              token_type_ids=None, \n",
        "                              attention_mask=test_input_mask)\n",
        "              \n",
        "\n",
        "          sentence_vecs_sum = outputs[0]\n",
        "\n",
        "          sentence_array = []\n",
        "          for i in range(0,len(sentence_vecs_sum)):\n",
        "            each_array = []\n",
        "            for j in range(0,len(sentence_vecs_sum[i])):\n",
        "              each_array.append(float(sentence_vecs_sum[i][j]))\n",
        "            sentence_array.append(each_array)\n",
        "\n",
        "          initial_df = pd.DataFrame(sentence_array)\n",
        "\n",
        "          from sklearn.manifold import TSNE\n",
        "          tsne = TSNE(n_components=2, random_state=0)\n",
        "          tsne_obj= tsne.fit_transform(initial_df)\n",
        "\n",
        "          tsne_df = pd.DataFrame({'X':tsne_obj[:,0],'Y':tsne_obj[:,1],'label':test_labels})\n",
        "\n",
        "          # f = open(\"/content/drive/MyDrive/\"+postposition+\"/Outcomes/\"+postposition+\"_accuracy_trial_\"+str(currentTry)+\"_epoch_\"+str(epochs)+\".txt\", 'w')\n",
        "          # tsne_df.to_csv(\"/content/drive/MyDrive/\"+postposition+\"_accuracy_trial_\"+str(currentTry)+\"_epoch_\"+str(epochs)+\".csv\")\n",
        "          # f = open(\"\"+postposition+\"/Outcomes/\"+postposition+\"_accuracy_trial_\"+str(currentTry)+\"_epoch_\"+str(epochs)+\".txt\", 'w')\n",
        "          # f.write(\"epoch,sentence,originallabel,predictedlabel,predictedFunction,result\"+\"\\n\")\n",
        "\n",
        "          import numpy as np   \n",
        "          import pandas as pd \n",
        "          from plotnine import *\n",
        "          import matplotlib.pyplot as plt\n",
        "\n",
        "          print(\"\")\n",
        "          print(\"  Network visualization  \")\n",
        "          print(ggplot(tsne_df, aes(x='X', y='Y')) + geom_point(aes(colour = 'label')))\n",
        "\n",
        "         \n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Training complete!\")\n",
        "      print(\"\")\n",
        "      print(\"Final result is below!\")\n",
        "      print(final_info)\n",
        "\n",
        "      # f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bCWZPMKFno_"
      },
      "outputs": [],
      "source": [
        "#Set Staart Time\n",
        "t0 = time.time()\n",
        "\n",
        "# Change to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "# Variable Initialize\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "# Import repeatedly by batch from the data loader\n",
        "for step, batch in enumerate(test_dataloader):  #15\n",
        "    if step % 100 == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    \n",
        "    with torch.no_grad():     \n",
        "      \n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "        \n",
        "    \n",
        "    logits = outputs[0]  #32 , 8\n",
        "\n",
        "    \n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "   \n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "print(\"\")\n",
        "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qLkJKTMF-lY"
      },
      "outputs": [],
      "source": [
        "#@title Create functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqn2YFtbALs_"
      },
      "outputs": [],
      "source": [
        "# Convert Input Data\n",
        "def convert_input_data(sentences):\n",
        "\n",
        "      # BERT Tokenize\n",
        "      tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "      # Max Length\n",
        "      MAX_LEN = setMaxLength\n",
        "\n",
        "      # Token to numeric index\n",
        "      input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "      \n",
        "      # Cut the sentence to MAX_LEN length and fill the missing part with padding 0\n",
        "      input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "      # Attention Mask Initialize\n",
        "      attention_masks = []\n",
        "\n",
        "      # Set the attention mask to 1 if it's not padding and 0 if it's padding\n",
        "      # The padding part does not perform attention on the BERT model, which speeds up\n",
        "      for seq in input_ids:\n",
        "          seq_mask = [float(i>0) for i in seq]\n",
        "          attention_masks.append(seq_mask)\n",
        "\n",
        "      # Convert data to tensors in the pytorch\n",
        "      inputs = torch.tensor(input_ids)\n",
        "      masks = torch.tensor(attention_masks)\n",
        "\n",
        "      return inputs, masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1vcCVQy9ij4"
      },
      "outputs": [],
      "source": [
        "# Test Sentence\n",
        "def test_sentences(sentences):\n",
        "\n",
        "    # Change to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    # Convert a sentence to input data\n",
        "    inputs, masks = convert_input_data(sentences)\n",
        "\n",
        "    # Input Data to GPU\n",
        "    b_input_ids = inputs.to(device)\n",
        "    b_input_mask = masks.to(device)            \n",
        "    \n",
        "    with torch.no_grad():     \n",
        "       \n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    # Get Lodd\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Input data to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLV_KKmXF8hi"
      },
      "outputs": [],
      "source": [
        "# Sentence Classify Test \n",
        "# 0: Target, 1: Intermediation, 2: Representation, 3: Vis_Var, 4: Vis_Tech\n",
        "\n",
        "logits = test_sentences(['The Bag is in my house.'])\n",
        "\n",
        "print(logits)\n",
        "print(np.argmax(logits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xUq_8ZVSsxq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "metaphor_BERT.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
